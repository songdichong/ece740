{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-12T13:30:39.181787Z",
     "end_time": "2023-04-12T13:30:41.683021Z"
    }
   },
   "outputs": [],
   "source": [
    "import click\n",
    "import torch\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.visualization.plot_images_grid import plot_images_grid\n",
    "from DeepSAD import DeepSAD\n",
    "from datasets.main import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-12T13:31:23.288913Z",
     "end_time": "2023-04-12T13:31:24.698618Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Log file is ../log/log.txt\n",
      "INFO:root:Data path is ../data\n",
      "INFO:root:Export path is ../log\n",
      "INFO:root:Dataset: cifar10\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.01\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.10\n",
      "INFO:root:Number of known anomaly classes: 9\n",
      "INFO:root:Network: ResNet_18\n",
      "INFO:root:Eta-parameter: 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Get configuration\n",
    "cfg = Config(locals().copy())\n",
    "xp_path = \"../log\"\n",
    "data_path = \"../data\"\n",
    "dataset_name = 'cifar10'\n",
    "normal_class = 0\n",
    "ratio_known_outlier = 0.01\n",
    "ratio_known_normal = 0\n",
    "ratio_pollution = 0.1\n",
    "n_known_outlier_classes = 9\n",
    "known_outlier_class = 1\n",
    "net_name = 'ResNet_18'\n",
    "load_config = None\n",
    "num_threads = 0\n",
    "n_jobs_dataloader = 0\n",
    "seed = 0 \n",
    "eta = 1.0\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "log_file = xp_path + '/log.txt'\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Print paths\n",
    "logger.info('Log file is %s' % log_file)\n",
    "logger.info('Data path is %s' % data_path)\n",
    "logger.info('Export path is %s' % xp_path)\n",
    "\n",
    "# Print experimental setup\n",
    "logger.info('Dataset: %s' % dataset_name)\n",
    "logger.info('Normal class: %d' % normal_class)\n",
    "logger.info('Ratio of labeled normal train samples: %.2f' % ratio_known_normal)\n",
    "logger.info('Ratio of labeled anomalous samples: %.2f' % ratio_known_outlier)\n",
    "logger.info('Pollution ratio of unlabeled train data: %.2f' % ratio_pollution)\n",
    "if n_known_outlier_classes == 1:\n",
    "    logger.info('Known anomaly class: %d' % known_outlier_class)\n",
    "else:\n",
    "    logger.info('Number of known anomaly classes: %d' % n_known_outlier_classes)\n",
    "    logger.info('Network: %s' % net_name)\n",
    "\n",
    "# If specified, load experiment config from JSON-file\n",
    "if load_config:\n",
    "    cfg.load_config(import_json=load_config)\n",
    "    logger.info('Loaded configuration from %s.' % load_config)\n",
    "\n",
    "# Print model configuration\n",
    "logger.info('Eta-parameter: %.2f' % eta)\n",
    "\n",
    "\n",
    "\n",
    "# Default device to 'cpu' if cuda is not available\n",
    "device = 'cuda'\n",
    "if not torch.cuda.is_available():\n",
    "    device = 'cpu'\n",
    "# Set the number of threads used for parallelizing CPU operations\n",
    "if num_threads > 0:\n",
    "    torch.set_num_threads(num_threads)\n",
    "    logger.info('Computation device: %s' % device)\n",
    "    logger.info('Number of threads: %d' % num_threads)\n",
    "    logger.info('Number of dataloader workers: %d' % n_jobs_dataloader)\n",
    "# Load data\n",
    "dataset = load_dataset(dataset_name, data_path, normal_class, known_outlier_class, n_known_outlier_classes,\n",
    "                        ratio_known_normal, ratio_known_outlier, ratio_pollution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdataset\u001B[49m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Known anomaly classes: (7, 9, 1, 3, 5, 4, 2, 6, 8)\n"
     ]
    }
   ],
   "source": [
    "# Log random sample of known anomaly classes if more than 1 class\n",
    "if n_known_outlier_classes > 1:\n",
    "    logger.info('Known anomaly classes: %s' % (dataset.known_outlier_classes,))\n",
    "\n",
    "# Initialize DeepSAD model and set neural network phi\n",
    "deepSAD = DeepSAD(eta)\n",
    "deepSAD.set_network(net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 100\n",
      "INFO:root:Pretraining learning rate scheduler milestones: [20, 50, 75]\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 1e-06\n",
      "INFO:root:Starting pretraining...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 32), <f4",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32mc:\\Users\\songd\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:3080\u001B[0m, in \u001B[0;36mfromarray\u001B[1;34m(obj, mode)\u001B[0m\n\u001B[0;32m   3079\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m-> 3080\u001B[0m     mode, rawmode \u001B[39m=\u001B[39m _fromarray_typemap[typekey]\n\u001B[0;32m   3081\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mKeyError\u001B[39;00m \u001B[39mas\u001B[39;00m e:\n",
      "\u001B[1;31mKeyError\u001B[0m: ((1, 1, 32), '<f4')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 15\u001B[0m\n\u001B[0;32m     12\u001B[0m logger\u001B[39m.\u001B[39minfo(\u001B[39m'\u001B[39m\u001B[39mPretraining weight decay: \u001B[39m\u001B[39m%g\u001B[39;00m\u001B[39m'\u001B[39m \u001B[39m%\u001B[39m ae_weight_decay)\n\u001B[0;32m     14\u001B[0m \u001B[39m# Pretrain model on dataset (via autoencoder)\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m deepSAD\u001B[39m.\u001B[39;49mpretrain(dataset,\n\u001B[0;32m     16\u001B[0m                     optimizer_name\u001B[39m=\u001B[39;49mae_optimizer_name,\n\u001B[0;32m     17\u001B[0m                     lr\u001B[39m=\u001B[39;49m ae_lr,\n\u001B[0;32m     18\u001B[0m                     n_epochs\u001B[39m=\u001B[39;49mae_n_epochs,\n\u001B[0;32m     19\u001B[0m                     lr_milestones\u001B[39m=\u001B[39;49mae_lr_milestone,\n\u001B[0;32m     20\u001B[0m                     batch_size\u001B[39m=\u001B[39;49mae_batch_size,\n\u001B[0;32m     21\u001B[0m                     weight_decay\u001B[39m=\u001B[39;49mae_weight_decay,\n\u001B[0;32m     22\u001B[0m                     device\u001B[39m=\u001B[39;49mdevice,\n\u001B[0;32m     23\u001B[0m                     n_jobs_dataloader\u001B[39m=\u001B[39;49mn_jobs_dataloader)\n\u001B[0;32m     25\u001B[0m \u001B[39m# Save pretraining results\u001B[39;00m\n\u001B[0;32m     26\u001B[0m deepSAD\u001B[39m.\u001B[39msave_ae_results(export_json\u001B[39m=\u001B[39mxp_path \u001B[39m+\u001B[39m \u001B[39m'\u001B[39m\u001B[39m/ae_results.json\u001B[39m\u001B[39m'\u001B[39m)\n",
      "File \u001B[1;32mc:\\Users\\songd\\Desktop\\ece740\\Deep-SAD-PyTorch\\src\\DeepSAD.py:101\u001B[0m, in \u001B[0;36mDeepSAD.pretrain\u001B[1;34m(self, dataset, optimizer_name, lr, n_epochs, lr_milestones, batch_size, weight_decay, device, n_jobs_dataloader)\u001B[0m\n\u001B[0;32m     97\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mae_optimizer_name \u001B[39m=\u001B[39m optimizer_name\n\u001B[0;32m     98\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mae_trainer \u001B[39m=\u001B[39m AETrainer(optimizer_name, lr\u001B[39m=\u001B[39mlr, n_epochs\u001B[39m=\u001B[39mn_epochs, lr_milestones\u001B[39m=\u001B[39mlr_milestones,\n\u001B[0;32m     99\u001B[0m                             batch_size\u001B[39m=\u001B[39mbatch_size, weight_decay\u001B[39m=\u001B[39mweight_decay, device\u001B[39m=\u001B[39mdevice,\n\u001B[0;32m    100\u001B[0m                             n_jobs_dataloader\u001B[39m=\u001B[39mn_jobs_dataloader)\n\u001B[1;32m--> 101\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mae_net \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mae_trainer\u001B[39m.\u001B[39;49mtrain(dataset, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mae_net)\n\u001B[0;32m    103\u001B[0m \u001B[39m# Get train results\u001B[39;00m\n\u001B[0;32m    104\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mae_results[\u001B[39m'\u001B[39m\u001B[39mtrain_time\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mae_trainer\u001B[39m.\u001B[39mtrain_time\n",
      "File \u001B[1;32mc:\\Users\\songd\\Desktop\\ece740\\Deep-SAD-PyTorch\\src\\optim\\ae_trainer.py:58\u001B[0m, in \u001B[0;36mAETrainer.train\u001B[1;34m(self, dataset, ae_net)\u001B[0m\n\u001B[0;32m     56\u001B[0m n_batches \u001B[39m=\u001B[39m \u001B[39m0\u001B[39m\n\u001B[0;32m     57\u001B[0m epoch_start_time \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime()\n\u001B[1;32m---> 58\u001B[0m \u001B[39mfor\u001B[39;00m data \u001B[39min\u001B[39;00m train_loader:\n\u001B[0;32m     59\u001B[0m     inputs, _, _, _ \u001B[39m=\u001B[39m data\n\u001B[0;32m     60\u001B[0m     inputs \u001B[39m=\u001B[39m inputs\u001B[39m.\u001B[39mto(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdevice)\n",
      "File \u001B[1;32mc:\\Users\\songd\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    631\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_sampler_iter \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m    632\u001B[0m     \u001B[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    633\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_reset()  \u001B[39m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 634\u001B[0m data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_next_data()\n\u001B[0;32m    635\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_num_yielded \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m \u001B[39m1\u001B[39m\n\u001B[0;32m    636\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_dataset_kind \u001B[39m==\u001B[39m _DatasetKind\u001B[39m.\u001B[39mIterable \u001B[39mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_IterableDataset_len_called \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39mand\u001B[39;00m \\\n\u001B[0;32m    638\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_num_yielded \u001B[39m>\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mc:\\Users\\songd\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    676\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_next_data\u001B[39m(\u001B[39mself\u001B[39m):\n\u001B[0;32m    677\u001B[0m     index \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_next_index()  \u001B[39m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 678\u001B[0m     data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_dataset_fetcher\u001B[39m.\u001B[39;49mfetch(index)  \u001B[39m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    679\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_pin_memory:\n\u001B[0;32m    680\u001B[0m         data \u001B[39m=\u001B[39m _utils\u001B[39m.\u001B[39mpin_memory\u001B[39m.\u001B[39mpin_memory(data, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mc:\\Users\\songd\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset\u001B[39m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[39m=\u001B[39m [\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset[idx] \u001B[39mfor\u001B[39;00m idx \u001B[39min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mc:\\Users\\songd\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset\u001B[39m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[39m=\u001B[39m [\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mdataset[idx] \u001B[39mfor\u001B[39;00m idx \u001B[39min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mc:\\Users\\songd\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    296\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39misinstance\u001B[39m(idx, \u001B[39mlist\u001B[39m):\n\u001B[0;32m    297\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset[[\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mindices[i] \u001B[39mfor\u001B[39;00m i \u001B[39min\u001B[39;00m idx]]\n\u001B[1;32m--> 298\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mdataset[\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mindices[idx]]\n",
      "File \u001B[1;32mc:\\Users\\songd\\Desktop\\ece740\\Deep-SAD-PyTorch\\src\\datasets\\cifar10.py:159\u001B[0m, in \u001B[0;36mMyCIFAR10.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    156\u001B[0m img, target, semi_target \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdata[index], \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mtargets[index], \u001B[39mint\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39msemi_targets[index])\n\u001B[0;32m    157\u001B[0m \u001B[39m# doing this so that it is consistent with all other datasets\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \u001B[39m# to return a PIL Image\u001B[39;00m\n\u001B[1;32m--> 159\u001B[0m img \u001B[39m=\u001B[39m Image\u001B[39m.\u001B[39;49mfromarray(img)\n\u001B[0;32m    160\u001B[0m \u001B[39m# try:\u001B[39;00m\n\u001B[0;32m    161\u001B[0m \u001B[39m#     img = Image.fromarray(img)\u001B[39;00m\n\u001B[0;32m    162\u001B[0m \u001B[39m# except Exception:\u001B[39;00m\n\u001B[0;32m    163\u001B[0m \u001B[39m#     pass\u001B[39;00m\n\u001B[0;32m    164\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mtransform \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n",
      "File \u001B[1;32mc:\\Users\\songd\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:3083\u001B[0m, in \u001B[0;36mfromarray\u001B[1;34m(obj, mode)\u001B[0m\n\u001B[0;32m   3081\u001B[0m     \u001B[39mexcept\u001B[39;00m \u001B[39mKeyError\u001B[39;00m \u001B[39mas\u001B[39;00m e:\n\u001B[0;32m   3082\u001B[0m         msg \u001B[39m=\u001B[39m \u001B[39m\"\u001B[39m\u001B[39mCannot handle this data type: \u001B[39m\u001B[39m%s\u001B[39;00m\u001B[39m, \u001B[39m\u001B[39m%s\u001B[39;00m\u001B[39m\"\u001B[39m \u001B[39m%\u001B[39m typekey\n\u001B[1;32m-> 3083\u001B[0m         \u001B[39mraise\u001B[39;00m \u001B[39mTypeError\u001B[39;00m(msg) \u001B[39mfrom\u001B[39;00m \u001B[39me\u001B[39;00m\n\u001B[0;32m   3084\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m   3085\u001B[0m     rawmode \u001B[39m=\u001B[39m mode\n",
      "\u001B[1;31mTypeError\u001B[0m: Cannot handle this data type: (1, 1, 32), <f4"
     ]
    }
   ],
   "source": [
    "ae_optimizer_name = \"adam\"\n",
    "ae_lr = 0.0001\n",
    "ae_n_epochs = 100\n",
    "ae_lr_milestone = [20, 50, 75]\n",
    "ae_batch_size = 128\n",
    "ae_weight_decay = 1e-6\n",
    "logger.info('Pretraining optimizer: %s' % ae_optimizer_name)\n",
    "logger.info('Pretraining learning rate: %g' % ae_lr)\n",
    "logger.info('Pretraining epochs: %d' % ae_n_epochs)\n",
    "logger.info('Pretraining learning rate scheduler milestones: %s' % (ae_lr_milestone,))\n",
    "logger.info('Pretraining batch size: %d' % ae_batch_size)\n",
    "logger.info('Pretraining weight decay: %g' % ae_weight_decay)\n",
    "\n",
    "# Pretrain model on dataset (via autoencoder)\n",
    "deepSAD.pretrain(dataset,\n",
    "                    optimizer_name=ae_optimizer_name,\n",
    "                    lr= ae_lr,\n",
    "                    n_epochs=ae_n_epochs,\n",
    "                    lr_milestones=ae_lr_milestone,\n",
    "                    batch_size=ae_batch_size,\n",
    "                    weight_decay=ae_weight_decay,\n",
    "                    device=device,\n",
    "                    n_jobs_dataloader=n_jobs_dataloader)\n",
    "\n",
    "# Save pretraining results\n",
    "deepSAD.save_ae_results(export_json=xp_path + '/ae_results.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
